{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from core.utils.logging import setup_logging\n",
    "setup_logging()\n",
    "import structlog\n",
    "logger = structlog.get_logger(__name__)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger.info(\"Loaded environment variables.\")"
   ],
   "id": "a852eee06f2c19f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from core.utils.shared import CONFIGS_DIR\n",
    "from core.config.config import ConfigManager\n",
    "\n",
    "from core.models.llm.model import LLMModel\n",
    "from core.models.lmv3.model import LMv3Model\n",
    "\n",
    "from core.data.utils import get_dataset\n",
    "from core.data.stats import evaluate_json_response\n",
    "from core.data.filters import filter_schematisms\n",
    "\n",
    "from core.data.schematism_parser import SchematismParser\n",
    "from core.data.translation_parser import Parser\n"
   ],
   "id": "a14640faf17dfcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "csv_path = \"/Volumes/T7/AI_Osrodek/data/csv/dane_hasla_with_filename.csv\"\n",
    "schematism_name = \"wloclawek_1872\""
   ],
   "id": "fa1c6db155776c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "schematism_parser = SchematismParser(csv_path=csv_path, schematism_name=schematism_name)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scanned_schematism = schematism_parser.schematisms_path / schematism_parser.schematism_name\n",
    "scanned_schematism"
   ],
   "id": "af2d41d2b97b716b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for file in os.listdir(scanned_schematism):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        results = schematism_parser.get_page_info(file)\n",
    "        print(\"_\"*15)\n",
    "        print(json.dumps(results, indent=4, ensure_ascii=False))\n",
    "\n",
    "        # with Image.open(scanned_schematism / file) as img:\n",
    "        #     image = img.copy()\n"
   ],
   "id": "75b1f111253c4175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "images_path = \"/Volumes/T7/Data/images\"",
   "id": "64e1918ff5bd9eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ],
   "id": "9e14c2ecaefcd9e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_parts(name: str):\n",
    "    parts = name.split(\"_\")\n",
    "\n",
    "    if len(parts) == 2:\n",
    "        return parts[0], parts[1]\n",
    "\n",
    "    # Detect schematism name based on known formats\n",
    "    if parts[1].isdigit():  # e.g. wloclawek_1872\n",
    "        schematism_name = \"_\".join(parts[:2])\n",
    "        filename = \"_\".join(parts[2:])\n",
    "    else:  # e.g. liber_crac_1529\n",
    "        schematism_name = \"_\".join(parts[:3])\n",
    "        filename = \"_\".join(parts[3:])\n",
    "\n",
    "    return schematism_name, filename\n",
    "\n"
   ],
   "id": "45a96ed56ce34343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pairs = []\n",
    "# schematism_parser = None\n",
    "# count = 0\n",
    "# for file in tqdm(os.listdir(images_path), desc=\"Processing images\"):\n",
    "#     if not file.endswith(\".jpg\"):\n",
    "#         continue\n",
    "#     with Image.open(os.path.join(images_path, file)) as img:\n",
    "#         image = img.copy()\n",
    "#     schematism_name, filename = extract_parts(file)\n",
    "#     # print(schematism_name,filename)\n",
    "#\n",
    "#     if schematism_parser is None:\n",
    "#         schematism_parser = SchematismParser(csv_path=csv_path, schematism_name=schematism_name)\n",
    "#     elif schematism_parser.schematism_name != schematism_name:\n",
    "#         schematism_parser = SchematismParser(csv_path=csv_path, schematism_name=schematism_name)\n",
    "#\n",
    "#     results = schematism_parser.get_page_info(filename)\n",
    "#\n",
    "#     pairs.append((image, results))\n",
    "#     if len(results[\"entries\"]) == 0:\n",
    "#         count += 1\n",
    "#         # display(image.resize((200,400)))"
   ],
   "id": "4ebd30f84dc25a5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def image_generator():\n",
    "    schematism_parser = None\n",
    "    for file in tqdm(os.listdir(images_path), desc=\"Processing images\"):\n",
    "        if not file.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        # Load image lazily\n",
    "        image_path = os.path.join(images_path, file)\n",
    "        schematism_name, filename = extract_parts(file)\n",
    "\n",
    "        if schematism_parser is None:\n",
    "            schematism_parser = SchematismParser(csv_path=csv_path, schematism_name=schematism_name)\n",
    "        elif schematism_parser.schematism_name != schematism_name:\n",
    "            schematism_parser = SchematismParser(csv_path=csv_path, schematism_name=schematism_name)\n",
    "\n",
    "        results = schematism_parser.get_page_info(filename)\n",
    "\n",
    "        # Yield dict instead of storing in memory\n",
    "        yield {\n",
    "            \"image\": image_path,  # Store path, not the image object\n",
    "            \"results\": results,\n",
    "            \"schematism_name\": schematism_name,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "\n",
    "# Create dataset from generator\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_generator(image_generator)"
   ],
   "id": "c4e46a8215d5d2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "names = [\n",
    "    \"liber_crac_1529_001.jgp\",\n",
    "    \"lodz_1872_0176_11.jgp\",\n",
    "    \"wloclawek_1872_001.jpg\",\n",
    "    ]\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "30531101cd85a8d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "37adeb9bf87afae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T20:08:52.969652Z",
     "start_time": "2025-07-09T20:06:58.795169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "def load_and_push_streaming(dataset, repo_name, batch_size=100):\n",
    "    \"\"\"\n",
    "    Load images in batches and push incrementally to avoid memory issues\n",
    "    \"\"\"\n",
    "    from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "    # Process and push in chunks\n",
    "    total_samples = len(dataset)\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(total_samples-1)//batch_size + 1}\")\n",
    "\n",
    "        # Get batch\n",
    "        batch_dataset = dataset.select(range(i, min(i + batch_size, total_samples)))\n",
    "\n",
    "        # Load images for this batch\n",
    "        def load_batch_images(example):\n",
    "            with Image.open(example[\"image\"]) as img:\n",
    "                image = img.copy()\n",
    "            del example[\"image\"]\n",
    "            example[\"image\"] = image\n",
    "            return example\n",
    "\n",
    "        batch_dataset = batch_dataset.map(load_batch_images)\n",
    "\n",
    "        # Push this batch\n",
    "\n",
    "        batch_dataset.push_to_hub(repo_name, split=\"train\")\n",
    "\n",
    "        # Clear memory\n",
    "        del batch_dataset\n",
    "        gc.collect()\n",
    "\n",
    "# Use streaming approach\n",
    "load_and_push_streaming(dataset, \"artpods56/sample_repo\", batch_size=50)"
   ],
   "id": "c0285d30859e6b5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e263184bb6642f194a46aa0612f2bcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1244fee9b8584c1b9f316b888f6ddc94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb7c3cc398a34cee872bab491b469ae5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m2025-07-09 20:07:01.545761\u001B[0m [\u001B[33m\u001B[1mwarning  \u001B[0m] \u001B[1mNo files have been modified since last commit. Skipping to prevent empty commit.\u001B[0m [\u001B[0m\u001B[1m\u001B[34mhuggingface_hub.hf_api\u001B[0m]\u001B[0m\n",
      "Processing batch 2/327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5c358f6931e46d7b7ff5e513c697f0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad27659475df4035a72c544112164773"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3/327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66f346ae7ae842d8802ea96954b55a75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cbad89612f74d3db4b84672ebfe7e8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32ff1ca4e71f4579bc23bf9103d82402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8745d657f70446d58d24d23f2b5bd370"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4/327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de16e9782c274b38b1921e0e3896c5f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32ba770218174900b52b2a4fc46a068e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f196693aea77464c8802ea6dfdc4a6d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86aef73da17c4a71872251440ad11f9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5/327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27883b494c4a41b4af8852140bce4cb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee6ddb4c8c1b44ff8c8591ad469b9e20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd2167bd74a240f08b87064ce9f4b5b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 37\u001B[39m\n\u001B[32m     34\u001B[39m         gc.collect()\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Use streaming approach\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[43mload_and_push_streaming\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43martpods56/sample_repo\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36mload_and_push_streaming\u001B[39m\u001B[34m(dataset, repo_name, batch_size)\u001B[39m\n\u001B[32m     26\u001B[39m batch_dataset = batch_dataset.map(load_batch_images)\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Push this batch\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[43mbatch_dataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpush_to_hub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# Clear memory\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m batch_dataset\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/datasets/arrow_dataset.py:5560\u001B[39m, in \u001B[36mDataset.push_to_hub\u001B[39m\u001B[34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001B[39m\n\u001B[32m   5557\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data_dir:\n\u001B[32m   5558\u001B[39m     data_dir = config_name \u001B[38;5;28;01mif\u001B[39;00m config_name != \u001B[33m\"\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# for backward compatibility\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m5560\u001B[39m additions, uploaded_size, dataset_nbytes = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_push_parquet_shards_to_hub\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5561\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5562\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5563\u001B[39m \u001B[43m    \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5564\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5565\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5566\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_shard_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_shard_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5567\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_shards\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_shards\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5568\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_pr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcreate_pr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5569\u001B[39m \u001B[43m    \u001B[49m\u001B[43membed_external_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43membed_external_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5570\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5572\u001B[39m \u001B[38;5;66;03m# Check if the repo already has a README.md and/or a dataset_infos.json to update them with the new split info (size and pattern)\u001B[39;00m\n\u001B[32m   5573\u001B[39m \u001B[38;5;66;03m# and delete old split shards (if they exist)\u001B[39;00m\n\u001B[32m   5574\u001B[39m repo_with_dataset_card, repo_with_dataset_infos = \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/datasets/arrow_dataset.py:5401\u001B[39m, in \u001B[36mDataset._push_parquet_shards_to_hub\u001B[39m\u001B[34m(self, repo_id, data_dir, split, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001B[39m\n\u001B[32m   5399\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m buffer\n\u001B[32m   5400\u001B[39m     shard_addition = CommitOperationAdd(path_in_repo=shard_path_in_repo, path_or_fileobj=parquet_content)\n\u001B[32m-> \u001B[39m\u001B[32m5401\u001B[39m     \u001B[43mapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpreupload_lfs_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5402\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5403\u001B[39m \u001B[43m        \u001B[49m\u001B[43madditions\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mshard_addition\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5404\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdataset\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   5405\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5406\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcreate_pr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcreate_pr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5407\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5408\u001B[39m     additions.append(shard_addition)\n\u001B[32m   5410\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m additions, uploaded_size, dataset_nbytes\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4483\u001B[39m, in \u001B[36mHfApi.preupload_lfs_files\u001B[39m\u001B[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001B[39m\n\u001B[32m   4478\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m has_binary_data:\n\u001B[32m   4479\u001B[39m             logger.warning(\n\u001B[32m   4480\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mUploading files as bytes or binary IO objects is not supported by Xet Storage. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4481\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mFalling back to HTTP upload.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4482\u001B[39m             )\n\u001B[32m-> \u001B[39m\u001B[32m4483\u001B[39m     \u001B[43m_upload_lfs_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mupload_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_threads\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_threads\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore [arg-type]\u001B[39;00m\n\u001B[32m   4484\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m addition \u001B[38;5;129;01min\u001B[39;00m new_lfs_additions_to_upload:\n\u001B[32m   4485\u001B[39m     addition._is_uploaded = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/_commit_api.py:453\u001B[39m, in \u001B[36m_upload_lfs_files\u001B[39m\u001B[34m(additions, repo_type, repo_id, headers, endpoint, num_threads, revision)\u001B[39m\n\u001B[32m    451\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(filtered_actions) == \u001B[32m1\u001B[39m:\n\u001B[32m    452\u001B[39m     logger.debug(\u001B[33m\"\u001B[39m\u001B[33mUploading 1 LFS file to the Hub\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m453\u001B[39m     \u001B[43m_wrapped_lfs_upload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiltered_actions\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    454\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    455\u001B[39m     logger.debug(\n\u001B[32m    456\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUploading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(filtered_actions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m LFS files to the Hub using up to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_threads\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m threads concurrently\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    457\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/_commit_api.py:443\u001B[39m, in \u001B[36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001B[39m\u001B[34m(batch_action)\u001B[39m\n\u001B[32m    441\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    442\u001B[39m     operation = oid2addop[batch_action[\u001B[33m\"\u001B[39m\u001B[33moid\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m--> \u001B[39m\u001B[32m443\u001B[39m     \u001B[43mlfs_upload\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m=\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlfs_batch_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    444\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    445\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError while uploading \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moperation.path_in_repo\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m to the Hub.\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/lfs.py:246\u001B[39m, in \u001B[36mlfs_upload\u001B[39m\u001B[34m(operation, lfs_batch_action, token, headers, endpoint)\u001B[39m\n\u001B[32m    242\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mValueError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m):\n\u001B[32m    243\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    244\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchunk_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    245\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m246\u001B[39m     \u001B[43m_upload_multi_part\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m=\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupload_url\u001B[49m\u001B[43m=\u001B[49m\u001B[43mupload_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    248\u001B[39m     _upload_single_part(operation=operation, upload_url=upload_url)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/lfs.py:346\u001B[39m, in \u001B[36m_upload_multi_part\u001B[39m\u001B[34m(operation, header, chunk_size, upload_url)\u001B[39m\n\u001B[32m    337\u001B[39m     warnings.warn(\n\u001B[32m    338\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mhf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    339\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m upload\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    340\u001B[39m     )\n\u001B[32m    341\u001B[39m     use_hf_transfer = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    343\u001B[39m response_headers = (\n\u001B[32m    344\u001B[39m     _upload_parts_hf_transfer(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)\n\u001B[32m    345\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m use_hf_transfer\n\u001B[32m--> \u001B[39m\u001B[32m346\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_upload_parts_iteratively\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m=\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msorted_parts_urls\u001B[49m\u001B[43m=\u001B[49m\u001B[43msorted_parts_urls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    347\u001B[39m )\n\u001B[32m    349\u001B[39m \u001B[38;5;66;03m# 3. Send completion request\u001B[39;00m\n\u001B[32m    350\u001B[39m completion_res = get_session().post(\n\u001B[32m    351\u001B[39m     upload_url,\n\u001B[32m    352\u001B[39m     json=_get_completion_payload(response_headers, operation.upload_info.sha256.hex()),\n\u001B[32m    353\u001B[39m     headers=LFS_HEADERS,\n\u001B[32m    354\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/lfs.py:403\u001B[39m, in \u001B[36m_upload_parts_iteratively\u001B[39m\u001B[34m(operation, sorted_parts_urls, chunk_size)\u001B[39m\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m part_idx, part_upload_url \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sorted_parts_urls):\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m SliceFileObj(\n\u001B[32m    398\u001B[39m         fileobj,\n\u001B[32m    399\u001B[39m         seek_from=chunk_size * part_idx,\n\u001B[32m    400\u001B[39m         read_limit=chunk_size,\n\u001B[32m    401\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m fileobj_slice:\n\u001B[32m    402\u001B[39m         \u001B[38;5;66;03m# S3 might raise a transient 500 error -> let's retry if that happens\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m         part_upload_res = \u001B[43mhttp_backoff\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mPUT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpart_upload_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfileobj_slice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_on_status_codes\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m502\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m503\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m504\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    406\u001B[39m         hf_raise_for_status(part_upload_res)\n\u001B[32m    407\u001B[39m         headers.append(part_upload_res.headers)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:310\u001B[39m, in \u001B[36mhttp_backoff\u001B[39m\u001B[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001B[39m\n\u001B[32m    307\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m].seek(io_obj_initial_pos)\n\u001B[32m    309\u001B[39m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m response = \u001B[43msession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m response.status_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m retry_on_status_codes:\n\u001B[32m    312\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/requests/sessions.py:589\u001B[39m, in \u001B[36mSession.request\u001B[39m\u001B[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[39m\n\u001B[32m    584\u001B[39m send_kwargs = {\n\u001B[32m    585\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m: timeout,\n\u001B[32m    586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mallow_redirects\u001B[39m\u001B[33m\"\u001B[39m: allow_redirects,\n\u001B[32m    587\u001B[39m }\n\u001B[32m    588\u001B[39m send_kwargs.update(settings)\n\u001B[32m--> \u001B[39m\u001B[32m589\u001B[39m resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/requests/sessions.py:703\u001B[39m, in \u001B[36mSession.send\u001B[39m\u001B[34m(self, request, **kwargs)\u001B[39m\n\u001B[32m    700\u001B[39m start = preferred_clock()\n\u001B[32m    702\u001B[39m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m r = \u001B[43madapter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[32m    706\u001B[39m elapsed = preferred_clock() - start\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001B[39m, in \u001B[36mUniqueRequestIdAdapter.send\u001B[39m\u001B[34m(self, request, *args, **kwargs)\u001B[39m\n\u001B[32m     94\u001B[39m     logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSend: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_curlify(request)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m requests.RequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     98\u001B[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/requests/adapters.py:667\u001B[39m, in \u001B[36mHTTPAdapter.send\u001B[39m\u001B[34m(self, request, stream, timeout, verify, cert, proxies)\u001B[39m\n\u001B[32m    664\u001B[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001B[32m    666\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m667\u001B[39m     resp = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    668\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    669\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    670\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    671\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    672\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    673\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    674\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    675\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    676\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    677\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    678\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    679\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    681\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    682\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request=request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    784\u001B[39m response_conn = conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    786\u001B[39m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m787\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    788\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    789\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    790\u001B[39m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    791\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    792\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    793\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    794\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    795\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    796\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    797\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    799\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    800\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    802\u001B[39m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[32m    803\u001B[39m clean_exit = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001B[39m, in \u001B[36mHTTPConnectionPool._make_request\u001B[39m\u001B[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[39m\n\u001B[32m    532\u001B[39m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[32m    533\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m534\u001B[39m     response = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    535\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    536\u001B[39m     \u001B[38;5;28mself\u001B[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/urllib3/connection.py:565\u001B[39m, in \u001B[36mHTTPConnection.getresponse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    562\u001B[39m _shutdown = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.sock, \u001B[33m\"\u001B[39m\u001B[33mshutdown\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m565\u001B[39m httplib_response = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    568\u001B[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/http/client.py:1430\u001B[39m, in \u001B[36mHTTPConnection.getresponse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1428\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1429\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1430\u001B[39m         \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1431\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[32m   1432\u001B[39m         \u001B[38;5;28mself\u001B[39m.close()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/http/client.py:331\u001B[39m, in \u001B[36mHTTPResponse.begin\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    329\u001B[39m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m331\u001B[39m     version, status, reason = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    332\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m status != CONTINUE:\n\u001B[32m    333\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/http/client.py:292\u001B[39m, in \u001B[36mHTTPResponse._read_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    291\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m292\u001B[39m     line = \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[33m\"\u001B[39m\u001B[33miso-8859-1\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    293\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) > _MAXLINE:\n\u001B[32m    294\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[33m\"\u001B[39m\u001B[33mstatus line\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/socket.py:720\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    718\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    719\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m720\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    721\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    722\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/ssl.py:1251\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1247\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1248\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1249\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1250\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1252\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1253\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ocr/lib/python3.12/ssl.py:1103\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1101\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1102\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1103\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1104\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1105\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda (ocr)",
   "language": "python",
   "name": "ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
