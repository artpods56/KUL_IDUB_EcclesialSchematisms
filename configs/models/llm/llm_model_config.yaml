predictor:
  api_type: "llama"
  template_dir: "prompts"
  max_retries: 5

interfaces:
  lm_studio:
    model: "gemma-3-27b-it"
    structured_output: true
    base_url: "http://localhost:1234/v1"
    api_key_env_var: "OPENAI_API_KEY"
    template_dir: "prompts"
    api_kwargs:
      max_tokens: 8096
      temperature: 0.1
      top_p: 0.9

  llama:
    model: "/models/gemma-3-27b-it-Q4_K_M.gguf"
    return_structured_output: true
    request_structured_output: true
    base_url: "http://localhost:8080/v1"
    api_key_env_var: "OPENAI_API_KEY"
    template_dir: "prompts"
    # api_kwargs: []

  openrouter:
#    model: "google/gemma-3-27b-it"
    model: "openai/gpt-4o"
    base_url: "https://openrouter.ai/api/v1"
    api_key_env_var: "OPENROUTER_API_KEY"
    structured_output: true
    api_kwargs:
      max_tokens: 4096
      temperature: 0.1
      top_p: 0.9

  openai:
    model: "gemma-3-27b-it-qat"
    api_key_env_var: "OPENAI_API_KEY"
    structured_output: true
    api_kwargs:
      max_tokens: 4096
      temperature: 0.1
      top_p: 0.9

  mistral:
    model: "mistral-large-latest"
    api_key_env_var: "MISTRAL_API_KEY"
    structured_output: true
    template_dir: "prompts"
    api_kwargs:
      max_tokens: 4096
      temperature: 0.1
      top_p: 0.9