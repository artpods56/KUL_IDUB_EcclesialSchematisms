model:
  checkpoint: microsoft/layoutlmv3-base

run:
  device: cpu

dataset:
  path: "artpods56/EcclesialSchematisms"
  name: "EcclesialSchematisms"
  force_download: false
  trust_remote_code: true
  keep_in_memory: true
  num_proc: 8
  seed: 42
  eval_size: 0.2
  test_size: 0.1
  split: "train"

  image_column_name: "image_pil"
  text_column_name: "words"
  boxes_column_name: "bboxes"
  label_column_name: "labels"

  schematisms_to_train:
    - "wloclawek_1872"
    - "wloclawek_1873"
    - "tarnow_1870"
    - "chelmno_1871"

  classes_to_remove:
    - "B-settlement_classification" 
    - "I-settlement_classification"
    - "B-material" 
    - "I-material"
    - "B-deanery"
    - "I-deanery"

processor:
  checkpoint: microsoft/layoutlmv3-large
  max_length: 512


focal_loss:
  alpha: 1.0
  gamma: 1.0

training:
  output_dir: "models/checkpoints/layoutlmv3-large-focal"
  max_steps: 100
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 2e-5
  eval_strategy: "steps"
  eval_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "eval_overall_f1"
  report_to: "wandb"
  run_name: "layoutlmv3-large-focal"
  logging_strategy: "steps"
  logging_steps: 100
  logging_dir: "logs_focal"
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 4
  fp16: true

inference:
  checkpoint: "models/checkpoints/layoutlmv3-large-focal/checkpoint-4000"
  apply_ocr: false

metrics:
  return_entity_level_metrics: true

defaults:
  - _self_
  - wandb
