{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d942b72",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6d17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForTokenClassification\n",
    "import torch\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from utils import sliding_window\n",
    "from core import LayoutLMv3Interface, SQLite3Interface\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "layoutlmv3_interface = LayoutLMv3Interface(\n",
    "    model_path=os.path.join(ROOT_DIR, \"models/layoutlmv3/checkpoint-400\"),\n",
    "    processor_path=\"microsoft/layoutlmv3-large\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "sqllite3_interface = SQLite3Interface(\n",
    "    database=os.path.join(ROOT_DIR, \"ecclesia.db\"),\n",
    ")\n",
    "\n",
    "SCHEMATISMS_DIR = os.path.join(ROOT_DIR, \"data/schematyzmy\")\n",
    "OCR_SCHEMATISMS_DIR = os.path.join(ROOT_DIR, \"data/ocr_schematyzmy\")\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"data/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb5e23",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb24d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens_with_labels(image_path, words, bboxes, labels, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize tokens with bounding boxes and colored labels on the image.\n",
    "    Ignores tokens with label \"O\".\n",
    "    Available labels: {'O', 'building_material', 'dedication', 'parish'}\n",
    "    \"\"\"\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "    # Define colors for each label\n",
    "    label2color = {\n",
    "        'building_material': 'red',\n",
    "        'dedication': 'orange',\n",
    "        'parish': 'blue',\n",
    "        'deanery': 'green',\n",
    "    }\n",
    "\n",
    "    # Open image\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"Arial\", 14)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        for word, bbox, label in zip(words, bboxes, labels):\n",
    "            if label == \"O\":\n",
    "                continue\n",
    "            color = label2color.get(label, \"black\")\n",
    "            draw.rectangle(bbox, outline=color, width=2)\n",
    "            # Draw label and word above the box\n",
    "            label_text = f\"{label}: {word}\"\n",
    "            # Use textbbox instead of textsize\n",
    "            left, top, right, bottom = draw.textbbox((0, 0), label_text, font=font)\n",
    "            text_width = right - left\n",
    "            text_height = bottom - top\n",
    "            text_x, text_y = bbox[0], max(0, bbox[1] - text_height)\n",
    "            draw.rectangle([text_x, text_y, text_x + text_width, text_y + text_height], fill=(255,255,255,180))\n",
    "            draw.text((text_x, text_y), label_text, fill=color, font=font)\n",
    "\n",
    "        if output_path:\n",
    "            img.save(output_path)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d2a53",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af2aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a specialized extraction assistant that identifies and labels specific information from 19th‑century Latin ecclesiastical schematisms (diocesan notices).\n",
    "\n",
    "Your task is to extract labeled text spans from Latin‑Polish diocesan notices and return them as a JSON list with positional information. You must align Latin terms in the source text with their Polish equivalents in the ground‑truth table.\n",
    "\n",
    "---\n",
    "\n",
    "## Labels and Their Meanings\n",
    "\n",
    "| Label                  | What to Extract                    | Latin Format Examples                                                                               | Polish Translation              |\n",
    "| ---------------------- | ---------------------------------- | --------------------------------------------------------------------------------------------------- | ------------------------------- |\n",
    "| **parish**             | Parish name                        | Usually capitalized; follows the ordinal number **or** appears as the first word in the notice line | Direct name (may vary slightly) |\n",
    "| **page_number**       | Page number in the schematism      | Usually numeric, often first in notice                                                              | Same number                     |\n",
    "| **dedication**         | Church's dedication / patron saint | `S.`, `SS.`, `B.M.V.`, `Nativ.` etc.                                                                | Full saint name in Polish       |\n",
    "| **building_material** | Church construction material       | `mur.`, `mr`, `murata`, `ex muro` (masonry) — `lig.`, `l.`, `dr`, `lignea` (wood)                   | `murowany` / `drewniany`        |\n",
    "| **deanery**            | Deanery name                       | Appears after `dekanat` or `decanatus`                                                              | Name in Polish                  |\n",
    "\n",
    "---\n",
    "\n",
    "## **Parsing Rules** (supersede earlier versions)\n",
    "\n",
    "1. **Notice boundaries**\n",
    "   *Ignore headings.* Textual headers such as `ECCLESIA Cathedr. …`, `PAROCHUS …`, etc. belong to the preceding context and **must be ignored** for extraction purposes. The *notice proper* begins at the first line that either:\n",
    "\n",
    "   * starts with an ordinal number (`\"1.\", \"2.\", \"—\"`, etc.), **or**\n",
    "   * starts with the parish name in its Polish nominative form followed by a comma.\n",
    "\n",
    "2. **Parish name (form preference)**\n",
    "\n",
    "   * Extract the **nominative form that appears verbatim** in the tokens, favouring the Polish spelling with diacritics (e.g., `Tarnów`).\n",
    "   * Disregard Latinised genitive/locative endings such as `-ae`, `-i`, `-ensis`, etc. (`Tarnoviae`, `Cracoviae`) **unless** no Polish nominative form occurs anywhere in the notice.\n",
    "\n",
    "3. **Ground‑truth alignment**\n",
    "\n",
    "   * If the Polish nominative form is present, extract that form so that it matches the Polish ground‑truth table.\n",
    "   * If only a Latin form is available, extract that Latin form **and** update the ground‑truth table accordingly (outside this assistant).\n",
    "\n",
    "4. **Other extraction constraints** \n",
    "\n",
    "   1. Extract **only** within the boundaries of a single notice.\n",
    "   2. Never split or merge tokens that are pre‑segmented in the input.\n",
    "   3. Match Latin terminology with corresponding Polish concepts in the ground truth.\n",
    "   4. Include positional information (word indices) for each extraction.\n",
    "   5. Omit any text that doesn't match the specific labels.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Few‑shot Examples\n",
    "# EXAMPLE 1\n",
    "### Ground Truth (Polish)\n",
    "[{'deanery': 'Dąbrowa', 'parish': 'Bolesław', 'dedication': 'Wojciech Biskup Męczennik', 'material_type': 'mr'}]\n",
    "### Input text\n",
    "[TEXT_START]\n",
    "In Circulo quondam Tarnoviensi.\n",
    "\n",
    "1. Decanatus Dabrovaensis.\n",
    "\n",
    "1. Bolesław, P. E. p. mur. — a. 1632 per Stanisl,\n",
    "Ligęza e ligno aedif. 22. Oct. 1634 per Thom. Oborski\n",
    "Ep. Laodicens. cons. dein desolata et combusta, — a. 1731\n",
    "mur. et per Joann. Skarbek Arcbi-Epp. Leopoliens. cons.\n",
    "Jam vero a. 1326 in opere per Aug. Theiner edito de hac\n",
    "ecclesia et curato mentio fit. T. E. S. Adalbertus E. M,\n",
    "Matr. ex a° 1648. Patr. T. D. Marianus Eques de Sroczyński.\n",
    "[TEXT_END]\n",
    "### OUTPUT\n",
    "[\n",
    "  {\"label\":\"deanery\", \"text\":\"Decanatus Dabrovaensis.\",  \"text_match_patch: \"In Circulo quondam Tarnoviensi.\\n1. <DEANERY>Decanatus Dabrovaensis.</DEANERY>\\n1. Bolesław\" ,\n",
    "  {\"label\":\"building_material\", \"text\":\"mur.\",  \"text_match_patch: \"combusta, — a. 1731\\n<BUILDING_MATERIAL>mur.</BUILDING_MATERIAL> et per\" ,\n",
    "  {\"label\":\"parish\", \"text\":\"Bolesław\", \"text_match_patch: \"Decanatus Dabrovaensis.\\n1. <PARISH>Bolesław</PARISH>, P. E.\" ,\n",
    "  {\"label\":\"dedication\", \"text\":\"T. E. S. Adalbertus E. M,\",  \"text_match_patch: \"mentio fit. <DEDICATION>T. E. S. Adalbertus E. M,</DEDICATION>\\nMatr.\" ,\n",
    "]\n",
    "\n",
    "# EXAMPLE 2\n",
    "### Ground Truth (Polish)\n",
    "\n",
    "Ground truth:  \n",
    "{'deanery': 'Dąbrowa', 'parish': 'Gręboszów', 'dedication': 'Najświętsza Maryja Panna Wniebowzięta', 'material_type': 'mr'}, \n",
    "{'deanery': 'Dąbrowa', 'parish': 'Dąbrowa', 'dedication': 'Wszyscy Święci', 'material_type': 'dr'}]\n",
    "### Input text\n",
    "[TEXT_START]\n",
    "140. Mẹdrzychów Y, 1240. Kupienin ®/, m. 460. — Univ.\n",
    "Cath. 6206. Jud, 435.\n",
    "\n",
    "Capitaneatus districtualis et off. postale Dąbrowa.\n",
    "\n",
    "2. Dabrowa, 0. E. p. lign. A. E. ign. olim praep.\n",
    "cum proprio Promot. SS. Rosarii. Eccl. antiqua per Nicolaum\n",
    "Spytek Ligęza Casteļlanum dotata, 1614 per Valerianum\n",
    "Lubieniecki Epp. Bacoviensem cons. ob vetustatem desolata,\n",
    "nova amplior 1774. per Cajetanum Potocki Canon. Cracov.\n",
    "de ligno extructa, per Gregorium Thomam Ziegler Epp. Ty-\n",
    "niec. 1824. cons. T. E. 00. SS, Matr. ant. ex a. 1611.\n",
    "Patr. T. D. Eugenius de Jordan Stojowski.\n",
    "Capitan. distr. et off. post. Dabrowa.\n",
    "\n",
    "3. Greboszów, P. E. p. mur. A. E. ignot. ast\n",
    "ante annum 1326, existens, juxta Theiner tom. I. p. 252.\n",
    "Praesens eccl. a Francisco de Dembiany Dembiński Palat,\n",
    "Cracov. 1650 aedificata, per Nicol. Oborski Epp. Laodicen.\n",
    "1675 in honorem Assumptionis B. M. V. cons. Matr. ant.\n",
    "Nator. -ex a. 165t. Patr. T. D. Sophia Comitissa Załuska,\n",
    "[TEXT_END]\n",
    "### OUTPUT\n",
    "[\n",
    "   {\"label\":\"parish\", \"text\":\"Dąbrowa\", \"text_match_patch: \"postale Dąbrowa. 2. <PARISH>Dabrowa</PARISH>, 0. E.\" ,\n",
    "   {\"label\":\"building_material\", \"text\":\"lign.\",  \"text_match_patch: \"E. p. <BUILDING_MATERIAL>lign.<BUILDING_MATERIAL> A. E. ign.\" ,\n",
    "   {\"label\":\"dedication\", \"text\":\"T. E. 00. SS.\",  \"text_match_patch: \"1824. cons. <DEDICATION>T. E. 00. SS.</DEDICATION> Matr. ant.\" ,\n",
    "   {\"label\":\"parish\", \"text\":\"Greboszów\", \"text_match_patch: \"post. Dabrowa. 3.<PARISH>Greboszów</PARISH>, P. E.\" ,\n",
    "   {\"label\":\"building_material\", \"text\":\"mur.\",  \"text_match_patch: \"E. p. <BUILDING_MATERIAL>mur.</BUILDING_MATERIAL> A. E. ignot.\" ,\n",
    "   {\"label\": \"dedication\", \"text\":\"Assumptionis B. M. V.\",  \"text_match_patch: \"in honorem <DEDICATION>Assumptionis B. M. V.</DEDICATION> cons.\" ,}\n",
    "]\n",
    "\n",
    "## END OF EXAMPLES\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04f2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "## User Prompt\n",
    "\n",
    "Extract and label information from this Latin ecclesiastical schematism that corresponds to the provided ground truth table in Polish.\n",
    "\n",
    "### Ground Truth (Polish)\n",
    "{ground_truth}\n",
    "\n",
    "\n",
    "### Instructions\n",
    "1. Identify Latin terms in the tokens that correspond to the Polish ground truth information.\n",
    "2. For \"material_type\", look for terms like \"mur.\", \"mr\", \"murata\" (for masonry) or \"lig.\", \"dr\", \"lignea\" (for wood).\n",
    "3. For \"dedication\", find Latin abbreviations like \"S.\", \"SS.\", \"B.M.V.\", \"Nativ.\" that represent the Polish saint/dedication.\n",
    "4. The \"parish\" name ussually is the same as the one in the ground truth table. It is very important to match the name with the token sequence.\n",
    "5. Return only a JSON array with labeled items - exclude anything that doesn't match the required labels.\n",
    "6. Dont output the ''' json beggin''' and '''json end''' tags.\n",
    "\n",
    "### Input text\n",
    "[TEXT_START]\n",
    "{input_text}\n",
    "[TEXT_END]\n",
    "\n",
    "I need a JSON output containing only the labeled information with text exactly as found in the tokens, remember to include the `text_match_patch` with valid text segment.. Response should include only the JSON array, nothing else. The ouput should be a valid JSON array.\n",
    "Return **only** a JSON array where each element is an object with:\n",
    "\n",
    "* `label`: one of the defined labels\n",
    "* `text`: the exact span as it appears in the input tokens (subject to Rule 2)\n",
    "* `text_match_patch`: the text span with `<LABEL>`…`</LABEL>` tags, sufficient to locate it in context, be sure to include the text before and after the match, **only one pair of tags per match**, should contain around 5 tokens before and after the match, but not more than 10 tokens in total.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17245551",
   "metadata": {},
   "source": [
    "# Pytesseract Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e1c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbaf1ce0",
   "metadata": {},
   "source": [
    "# LLM Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dda85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "import logging\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, client, model_name: str,  system_prompt: str, user_prompt: str, model_parameters: dict = None, stream: bool = False, no_think: bool = False, verbose: bool = False):\n",
    "        \n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        ]\n",
    "        self.model_parameters = model_parameters\n",
    "        self.no_think = no_think\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('[%(levelname)s] %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "        \n",
    "\n",
    "    def stream_response(self):\n",
    "        self.logger.info(\"\\n\" + \"-\"*20 + \" Start of streamed response \" + \"-\"*20)\n",
    "        if isinstance(self.client, OpenAI):\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=self.messages,\n",
    "                **self.model_parameters\n",
    "            )\n",
    "        elif isinstance(self.client, Mistral):\n",
    "            response = self.client.chat.stream(\n",
    "                model=self.model_name,\n",
    "                messages=self.messages,\n",
    "                **self.model_parameters\n",
    "            )\n",
    "        \n",
    "        complete_response = \"\"\n",
    "        for streamed_response in response:\n",
    "            if isinstance(self.client, OpenAI):\n",
    "                streamed_chunk = streamed_response.choices[0].delta.content\n",
    "            elif isinstance(self.client, Mistral):\n",
    "                streamed_chunk = streamed_response.data.choices[0].delta.content\n",
    "\n",
    "            if streamed_chunk:\n",
    "                if self.verbose:\n",
    "                    print(streamed_chunk, end=\"\", flush=True)\n",
    "                complete_response += streamed_chunk\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\" \", end=\"\", flush=True) \n",
    "        self.logger.info(\"\\n\" + \"-\"*20 + \" End of streamed response \" + \"-\"*20)\n",
    "\n",
    "        return complete_response\n",
    "    \n",
    "    def completions_response(self):\n",
    "        self.logger.info(\"\\n\" + \"-\"*20 + \" Start of non-streamed response \" + \"-\"*20)\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                if isinstance(self.client, Mistral):\n",
    "                    response = self.client.chat.complete(\n",
    "                        model=self.model_name,\n",
    "                        messages=self.messages,\n",
    "                        **self.model_parameters\n",
    "                    )\n",
    "                    response_content = response.choices[0].message.content\n",
    "                    break\n",
    "                elif isinstance(self.client, OpenAI):\n",
    "\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model_name,\n",
    "                        messages=self.messages,\n",
    "                        **self.model_parameters\n",
    "                    )\n",
    "                    response_content = response.choices[0].message.content\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error generating response: {e}\")\n",
    "                if i == 2:\n",
    "                    raise\n",
    "        self.logger.info(response_content)\n",
    "        self.logger.info(\"\\n\" + \"-\"*20 + \" End of non-streamed response \" + \"-\"*20)\n",
    "        return response_content\n",
    "\n",
    "    def process_response(self, response):\n",
    "        import re\n",
    "        import json\n",
    "        # get rid of the <think> and </think> tags\n",
    "        json_response = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL | re.MULTILINE).strip()\n",
    "        return json.loads(json_response)\n",
    "\n",
    "    def generate(self):\n",
    "        self.logger.info(f\"Model name: {self.model_name} - {self.client.__class__.__name__}\")\n",
    "        if self.model_parameters[\"stream\"]: \n",
    "            response = self.stream_response()\n",
    "        else:\n",
    "            response = self.completions_response()\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def get_labeles(self, page_ocr: str, ground_truth: str):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": self.user_prompt.format(input_text=page_ocr, ground_truth=ground_truth)})\n",
    "        if self.no_think:\n",
    "            self.messages[-1][\"content\"] += \"/no_think\"\n",
    "        response = self.generate()\n",
    "        \n",
    "        json_response = self.process_response(response)\n",
    "        self.logger.info(f\"Json response: {json_response}\")\n",
    "        return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9798c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8889f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing trailing punctuation\n",
    "    3. Removing diacritics\n",
    "    4. Standardizing whitespace\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove diacritics (normalize unicode characters)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Remove trailing punctuation and standardize whitespace\n",
    "    text = re.sub(r'[.,;:]+$', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find_fuzzy_span_indices(span, tokens, context=None, threshold=60):\n",
    "    \"\"\"Find token indices that best match the span, using context if provided.\"\"\"\n",
    "    # Normalize the span and its tokens\n",
    "    span_normalized = normalize_text(span)\n",
    "    span_tokens = span_normalized.split()\n",
    "    n = len(span_tokens)\n",
    "    \n",
    "    # Normalize all input tokens\n",
    "    tokens_normalized = [normalize_text(t) for t in tokens]\n",
    "    \n",
    "    best_score = -1\n",
    "    best_indices = []\n",
    "\n",
    "    # Slide through tokens looking for matches\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        window = tokens[i:i+n]\n",
    "        window_normalized = \" \".join(tokens_normalized[i:i+n])\n",
    "        \n",
    "        # Calculate base similarity score using normalized text\n",
    "        score = fuzz.ratio(span_normalized, window_normalized)\n",
    "        \n",
    "        # Check context if provided\n",
    "        if context and score >= threshold:\n",
    "            before, after = context\n",
    "            before_normalized = normalize_text(before)\n",
    "            after_normalized = normalize_text(after)\n",
    "            \n",
    "            if before:\n",
    "                before_window = \" \".join(tokens_normalized[max(0, i-len(before.split())):i])\n",
    "                if fuzz.ratio(before_normalized, before_window) < 70:  # Stricter threshold\n",
    "                    score -= 20\n",
    "            \n",
    "            if after:\n",
    "                after_window = \" \".join(tokens_normalized[i+n:i+n+len(after.split())])\n",
    "                if fuzz.ratio(after_normalized, after_window) < 70:\n",
    "                    score -= 20\n",
    "\n",
    "        if score > best_score and score >= threshold:\n",
    "            best_score = score\n",
    "            best_indices = list(range(i, i+n))\n",
    "\n",
    "    return best_indices\n",
    "\n",
    "def extract_context_from_patch(patch, span):\n",
    "    \"\"\"Extract normalized context before and after the labeled span.\"\"\"\n",
    "    before = after = \"\"\n",
    "    match = re.search(r'(.*)<[^>]*>%s</[^>]*>(.*)' % re.escape(span), patch, flags=re.S)\n",
    "    if match:\n",
    "        # Get context words (up to 3 words before and after)\n",
    "        before_words = match.group(1).strip().split()\n",
    "        after_words = match.group(2).strip().split()\n",
    "        \n",
    "        before = \" \".join(before_words)\n",
    "        after = \" \".join(after_words)\n",
    "    \n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b58d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lm_studio_client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\"       # LM Studio doesn’t enforce a real key\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labeler_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"annotations\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"label\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"parish\", \"building_material\", \"dedication\", \"deanery\", \"page_number\"]\n",
    "                    },\n",
    "                    \"text\": {\"type\": \"string\"},\n",
    "                    \"text_match_patch\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"label\", \"text\", \"text_match_patch\"]\n",
    "            },\n",
    "            \"minItems\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "lm_studio_parameters = {\n",
    "    # \"temperature\": 0.0,\n",
    "    # \"top_p\": 0.9,\n",
    "    # \"stop\": [\"\\n\\n\"],\n",
    "    \"stream\": True,\n",
    "    \"response_format\": labeler_schema,\n",
    "}\n",
    "\n",
    "# llm = LLM(client=lm_studio_client,\n",
    "#           model_name=\"qwen3-30b-a3b-mlx\",\n",
    "#           system_prompt=system_prompt,\n",
    "#           user_prompt=user_prompt,\n",
    "#           model_parameters=lm_studio_parameters,\n",
    "#           verbose=True,\n",
    "#           no_think=True,\n",
    "#           )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026511e6",
   "metadata": {},
   "source": [
    "# Client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099736c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_client = Mistral(api_key=\"\")\n",
    "\n",
    "mistral_parameters = {\n",
    "    \"temperature\": 0.5,\n",
    "    # \"frequency_penalty\": 1.2,  # Add this\n",
    "    # \"presence_penalty\": 0.6, \n",
    "    # \"top_p\": 0.9,\n",
    "    # \"stop\": [\"\\n\\n\"],\n",
    "    \"stream\": True,\n",
    "    \"response_format\": {\n",
    "          \"type\": \"json_object\",\n",
    "      }\n",
    "}\n",
    "\n",
    "llm = LLM(client=mistral_client,\n",
    "          model_name=\"mistral-large-latest\",\n",
    "          system_prompt=system_prompt,\n",
    "          user_prompt=user_prompt,\n",
    "          model_parameters=mistral_parameters,\n",
    "          verbose=True,\n",
    "          no_think=True,\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc661a0",
   "metadata": {},
   "source": [
    "# Annotation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e917f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "schematisms_to_evaluate = [\"wloclawek_1872\"]\n",
    "LLM_ANNOTATIONS_DIR = os.path.join(ROOT_DIR, \"data/llm_annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7bb2b",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce763e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442059c5",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89982859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT id, dekanat, miejsce, wezwanie, material_typ, the_geom, skany FROM dane_hasla WHERE skany = 'wloclawek_1872'\n"
     ]
    },
    {
     "ename": "DataSourceError",
     "evalue": "/Users/user/Projects/AI_Osrodek/data/schematyzmy/wloclawek_1872/matryca/matryca.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m geometries = sqllite3_interface.query([\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdekanat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmiejsce\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwezwanie\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmaterial_typ\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mthe_geom\u001b[39m\u001b[33m\"\u001b[39m , \u001b[33m\"\u001b[39m\u001b[33mskany\u001b[39m\u001b[33m\"\u001b[39m], {\u001b[33m\"\u001b[39m\u001b[33mskany\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschematism\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m}, table_name=\u001b[33m\"\u001b[39m\u001b[33mdane_hasla\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m shapefile_path = os.path.join(SCHEMATISMS_DIR, schematism, \u001b[33m\"\u001b[39m\u001b[33mmatryca/matryca.shp\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m shp_gdf = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapefile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wkt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/geopandas/io/file.py:294\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m             from_bytes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/geopandas/io/file.py:547\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m     warnings.warn(\n\u001b[32m    539\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    540\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    544\u001b[39m     )\n\u001b[32m    545\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/pyogrio/geopandas.py:265\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    263\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    264\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    285\u001b[39m     meta, table = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ocr/lib/python3.12/site-packages/pyogrio/raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1240\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:220\u001b[39m, in \u001b[36mpyogrio._io.ogr_open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDataSourceError\u001b[39m: /Users/user/Projects/AI_Osrodek/data/schematyzmy/wloclawek_1872/matryca/matryca.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "for schematism in schematisms_to_evaluate:\n",
    "    geometries = sqllite3_interface.query([\"id\", \"dekanat\", \"miejsce\", \"wezwanie\", \"material_typ\", \"the_geom\" , \"skany\"], {\"skany\": f\"'{schematism}'\"}, table_name=\"dane_hasla\")\n",
    "    shapefile_path = os.path.join(SCHEMATISMS_DIR, schematism, \"matryca/matryca.shp\")\n",
    "\n",
    "    shp_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "\n",
    "    geom_list = []\n",
    "    for geom in geometries:\n",
    "        _, deanery, parish, dedication, material_type, geom, schematism = geom\n",
    "\n",
    "        try:\n",
    "            geom = wkt.loads(geom)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading WKT: {geom}\")\n",
    "            logger.error(str(e))\n",
    "            continue\n",
    "        geom_list.append({\n",
    "            \"deanery\": deanery,\n",
    "            \"parish\": parish,\n",
    "            \"dedication\": dedication,\n",
    "            \"material_type\": material_type,\n",
    "            \"geom\": geom,\n",
    "            \"schematism\": schematism\n",
    "        })\n",
    "        logger.debug(f\"Geometry loaded: {geom}\")\n",
    "\n",
    "    sql_gdf = gpd.GeoDataFrame(geom_list, geometry=\"geom\", crs=shp_gdf.crs)\n",
    "\n",
    "    joined_gdf = gpd.sjoin(shp_gdf, sql_gdf, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    for file_name in sorted(os.listdir(os.path.join(SCHEMATISMS_DIR, schematism))):\n",
    "        annotation_file = os.path.join(LLM_ANNOTATIONS_DIR, schematism, file_name.replace(\".jpg\", \".json\"))\n",
    "        if os.path.exists(annotation_file):\n",
    "            logger.info(f\"Skipping existing annotation file: {annotation_file}\")\n",
    "            continue\n",
    "            \n",
    "        pytesseract_ocr = PytesseractOCRInterface(\n",
    "            schematisms_source_dir=SCHEMATISMS_DIR,\n",
    "            schematisms_ocr_target_dir=OCR_SCHEMATISMS_DIR,\n",
    "            schematism=schematism,\n",
    "            langs=[\"pol\", \"lat\"],\n",
    "            force_ocr=False,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        if not file_name.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(SCHEMATISMS_DIR, schematism, file_name)\n",
    "        page_ground_truth = joined_gdf[joined_gdf[\"location\"] == file_name]\n",
    "        \n",
    "        if not page_ground_truth.empty:\n",
    "            logger.info(f\"\\n{'='*80}\\nProcessing {file_name}...\\n{'='*80}\")\n",
    "\n",
    "            ocr_data = pytesseract_ocr.load_ocr_data(file_name)\n",
    "            tokens = ocr_data[\"words\"]\n",
    "            labels = [\"O\"] * len(ocr_data[\"words\"])\n",
    "            \n",
    "            # Clean up ground truth data\n",
    "            clean_ground_truth = page_ground_truth.drop(columns=[\"geometry\", \"location\", \"index_right\", \"schematism\"]).to_markdown(index=False)\n",
    "            logger.info(f\"\\nGround Truth:\\n{'-'*40}\\n{clean_ground_truth}\\n{'-'*40}\")\n",
    "            logger.info(f\"\\nOCR Text:\\n{'-'*40}\\n{ocr_data['complete_text']}\\n{'-'*40}\")\n",
    "            \n",
    "            labels_json = llm.get_labeles(page_ocr=ocr_data[\"complete_text\"], ground_truth=clean_ground_truth)\n",
    "            llm.messages.pop(-1)  # Remove the last user message\n",
    "            \n",
    "\n",
    "\n",
    "            for annotation in labels_json:\n",
    "\n",
    "                if isinstance(annotation, str):\n",
    "                    try:\n",
    "                        annotation = json.loads(annotation.strip())\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.error(f\"Error decoding JSON response: {e}\")\n",
    "                        continue\n",
    "\n",
    "                label = annotation[\"label\"]\n",
    "                patch = annotation[\"text_match_patch\"]\n",
    "                \n",
    "                span = re.search(r'<[^>]*>(.*?)</', patch, flags=re.S)\n",
    "                if not span:\n",
    "                    logger.error(f\"Error: No span found in patch: {patch}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    span = span.group(1)\n",
    "                    \n",
    "                # Extract context and find matching tokens\n",
    "                before, after = extract_context_from_patch(patch, span)\n",
    "                matching_indices = find_fuzzy_span_indices(span, tokens, context=(before, after))\n",
    "                \n",
    "                # Apply labels to matching tokens\n",
    "                for idx in matching_indices:\n",
    "                    labels[idx] = label\n",
    "\n",
    "            if not os.path.exists(os.path.join(LLM_ANNOTATIONS_DIR, schematism)):\n",
    "                os.makedirs(os.path.join(LLM_ANNOTATIONS_DIR, schematism))\n",
    "                \n",
    "\n",
    "            annotated_image = visualize_tokens_with_labels(image_path, ocr_data[\"words\"], ocr_data[\"bboxes\"], labels, output_path=None)\n",
    "            display(annotated_image)\n",
    "\n",
    "            with open(annotation_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                content = {\n",
    "                    \"words\": tokens,\n",
    "                    \"bboxes\": ocr_data[\"bboxes\"],\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "                json.dump(content, f, ensure_ascii=False, indent=4)\n",
    "                logger.info(f\"\\nAnnotation file saved: {annotation_file}\")\n",
    "        else:\n",
    "            logger.debug(f\"No ground truth data found for {file_name}\")\n",
    "            continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
