{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk.client import LabelStudio\n",
    "from label_studio_sdk.data_manager import Filters, Column, Operator\n",
    "from itertools import islice\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LS_URL      = \"https://ls.kpi.kul.pl/\"\n",
    "API_TOKEN   = \"e576444cc03ec6e7abf9d86f441523f1b7a6efb3\"\n",
    "PROJECT_ID  = 1                          # <- replace\n",
    "CHUNK_SIZE  = 200                         # keep the request small\n",
    "\n",
    "client = LabelStudio(base_url=LS_URL, api_key=API_TOKEN)\n",
    "project = client.projects.get(PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import label_studio_sdk\n",
    "label_studio_sdk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk import Client\n",
    "from label_studio_sdk.data_manager import Filters, Column, Operator, Type\n",
    "\n",
    "ls = Client(url=LS_URL, api_key=API_TOKEN)\n",
    "project = ls.get_project(PROJECT_ID)\n",
    "\n",
    "# filters = Filters.create(\n",
    "#     Filters.AND,\n",
    "#     [\n",
    "#         Filters.item(\n",
    "#             # works only on LS 1.11+\n",
    "#             Column.data(\"id\"),          # or just the string 'tasks:storage_filename'\n",
    "#             Operator.CONTAINS,                # EQUAL / REGEX / etc. also work\n",
    "#             Type.String,\n",
    "#             Filters.value(\"2024/05/my_image.jpg\")\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "filters = Filters.create( Filters.AND, [Filters.item( \"tasks:storage_filename\", Operator.CONTAINS, Type.String, Filters.value(\"chelmno_1871\") )])\n",
    "\n",
    "tasks = project.get_tasks(filters=filters)\n",
    "print(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens_with_labels(image_path, words, bboxes, labels, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize tokens with bounding boxes and colored labels on the image.\n",
    "    Ignores tokens with label \"O\".\n",
    "    Available labels: {'O', 'building_material', 'dedication', 'parish'}\n",
    "    \"\"\"\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "    # Define colors for each label\n",
    "    label2color = {\n",
    "        'building_material': 'red',\n",
    "        'dedication': 'orange',\n",
    "        'parish': 'blue',\n",
    "        'deanery': 'green',\n",
    "    }\n",
    "\n",
    "    # Open image\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"Arial\", 14)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        for word, bbox, label in zip(words, bboxes, labels):\n",
    "            if label == \"O\":\n",
    "                continue\n",
    "            color = label2color.get(label, \"black\")\n",
    "            draw.rectangle(bbox, outline=color, width=2)\n",
    "            # Draw label and word above the box\n",
    "            label_text = f\"{label}: {word}\"\n",
    "            # Use textbbox instead of textsize\n",
    "            left, top, right, bottom = draw.textbbox((0, 0), label_text, font=font)\n",
    "            text_width = right - left\n",
    "            text_height = bottom - top\n",
    "            text_x, text_y = bbox[0], max(0, bbox[1] - text_height)\n",
    "            draw.rectangle([text_x, text_y, text_x + text_width, text_y + text_height], fill=(255,255,255,180))\n",
    "            draw.text((text_x, text_y), label_text, fill=color, font=font)\n",
    "\n",
    "        if output_path:\n",
    "            img.save(output_path)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_dict = {task[\"storage_filename\"].split(\"/\")[-1]: task for task in tasks}\n",
    "tasks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_bio_entities(bboxes, labels, tokens, verbose=False):\n",
    "    \"\"\"\n",
    "    Merge consecutive entity tokens into single bounding boxes and tokens,\n",
    "    avoiding merging across apparent line breaks or large gaps.\n",
    "\n",
    "    Args:\n",
    "        bboxes (list): List of bounding boxes [x1, y1, x2, y2].\n",
    "        labels (list): List of label strings (e.g., \"parish\", \"deanery\", \"O\").\n",
    "        tokens (list): List of tokens/words.\n",
    "        verbose (bool): If True, print debug info.\n",
    "\n",
    "    Returns:\n",
    "        merged_boxes (list): List of merged bounding boxes.\n",
    "        merged_tokens (list): List of merged entity strings.\n",
    "        merged_classes (list): List of merged entity class names (e.g., 'parish').\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not (len(bboxes) == len(labels) == len(tokens)):\n",
    "        raise ValueError(\"bboxes, labels, and tokens must have the same length\")\n",
    "\n",
    "    merged_boxes = []\n",
    "    merged_tokens = []\n",
    "    merged_classes = []\n",
    "\n",
    "    bbox_stack = []\n",
    "    token_stack = []\n",
    "    current_entity_type = None\n",
    "    \n",
    "    def merge_entity(stack_bboxes, stack_tokens, entity_type):\n",
    "        \"\"\"Merge stacked boxes and tokens into a single entity.\"\"\"\n",
    "        valid_bboxes_np = np.array(stack_bboxes)\n",
    "        merged_bbox = [\n",
    "            np.min(valid_bboxes_np[:, 0]), # min x1\n",
    "            np.min(valid_bboxes_np[:, 1]), # min y1\n",
    "            np.max(valid_bboxes_np[:, 2]), # max x2\n",
    "            np.max(valid_bboxes_np[:, 3]), # max y2\n",
    "        ]\n",
    "        merged_token = \" \".join(stack_tokens)\n",
    "        return merged_bbox, merged_token, entity_type\n",
    "\n",
    "    for i, (bbox, label, token) in enumerate(zip(bboxes, labels, tokens)):\n",
    "        # Case 1: Current token is \"O\" (outside)\n",
    "        if label == \"O\":\n",
    "            if bbox_stack:  # Finalize the previous entity\n",
    "                merged_bbox, merged_token, entity_class = merge_entity(\n",
    "                    bbox_stack, token_stack, current_entity_type\n",
    "                )\n",
    "                if merged_bbox:\n",
    "                    merged_boxes.append(merged_bbox)\n",
    "                    merged_tokens.append(merged_token)\n",
    "                    merged_classes.append(entity_class)\n",
    "                    if verbose: print(f\"Finalized entity (due to O): {entity_class} -> '{merged_token}'\")\n",
    "                bbox_stack = []\n",
    "                token_stack = []\n",
    "                current_entity_type = None\n",
    "            # Do nothing else for \"O\"\n",
    "            \n",
    "        # Case 2: Current token has a label (entity)\n",
    "        else:\n",
    "            # If this is a new entity or different from the current one\n",
    "            if not bbox_stack or current_entity_type != label:\n",
    "                # Finalize any previous entity first\n",
    "                if bbox_stack:\n",
    "                    merged_bbox, merged_token, entity_class = merge_entity(\n",
    "                        bbox_stack, token_stack, current_entity_type\n",
    "                    )\n",
    "                    if merged_bbox:\n",
    "                        merged_boxes.append(merged_bbox)\n",
    "                        merged_tokens.append(merged_token)\n",
    "                        merged_classes.append(entity_class)\n",
    "                        if verbose: print(f\"Finalized entity (new entity): {entity_class} -> '{merged_token}'\")\n",
    "                \n",
    "                # Start new entity\n",
    "                bbox_stack = [bbox]\n",
    "                token_stack = [str(token)]  # Ensure token is string\n",
    "                current_entity_type = label\n",
    "                if verbose:\n",
    "                    print(f\"Starting new entity: {current_entity_type}, token: {token}\")\n",
    "                \n",
    "            # Continuing the same entity type\n",
    "            else:\n",
    "                # Check for line break or large gap before continuing\n",
    "                last_bbox = bbox_stack[-1]\n",
    "                current_bbox = bbox\n",
    "                \n",
    "                # Heuristic thresholds (adjust as needed)\n",
    "                max_vertical_dist_factor = 0.7  # Allow center diff up to 70% of last box height\n",
    "                max_horizontal_gap_factor = 2.0  # Allow gap up to 2.0x width of last box\n",
    "                \n",
    "                last_height = last_bbox[3] - last_bbox[1]\n",
    "                last_width = last_bbox[2] - last_bbox[0]\n",
    "                last_center_y = (last_bbox[1] + last_bbox[3]) / 2\n",
    "                current_center_y = (current_bbox[1] + current_bbox[3]) / 2\n",
    "                \n",
    "                vertical_dist = abs(current_center_y - last_center_y)\n",
    "                horizontal_gap = current_bbox[0] - last_bbox[2]  # Positive -> gap, Negative -> overlap\n",
    "                \n",
    "                is_break = False\n",
    "                # 1. Check for significant vertical distance (likely new line)\n",
    "                if last_height > 1 and vertical_dist > last_height * max_vertical_dist_factor:\n",
    "                    is_break = True\n",
    "                    if verbose: print(f\"Break detected (Vertical): Entity {current_entity_type}, Token '{token}', VDist: {vertical_dist:.1f} > Threshold: {last_height * max_vertical_dist_factor:.1f}\")\n",
    "                # 2. Check for large horizontal gap (only if vertically close)\n",
    "                elif last_width > 1 and horizontal_gap > last_width * max_horizontal_gap_factor and vertical_dist <= last_height * max_vertical_dist_factor:\n",
    "                    is_break = True\n",
    "                    if verbose: print(f\"Break detected (Horizontal Gap): Entity {current_entity_type}, Token '{token}', HGap: {horizontal_gap:.1f} > Threshold: {last_width * max_horizontal_gap_factor:.1f}\")\n",
    "                # 3. Check for wrap-around (significant move left AND down)\n",
    "                elif last_width > 1 and current_bbox[0] < (last_bbox[0] - last_width * 0.5) and current_center_y > last_center_y + last_height * 0.1:\n",
    "                    is_break = True\n",
    "                    if verbose: print(f\"Break detected (Wrap Around): Entity {current_entity_type}, Token '{token}', X1_curr: {current_bbox[0]:.1f} < Threshold: {last_bbox[0] - last_width * 0.5:.1f}\")\n",
    "                \n",
    "                if not is_break:\n",
    "                    # Continue the current entity\n",
    "                    bbox_stack.append(current_bbox)\n",
    "                    token_stack.append(str(token))\n",
    "                    if verbose:\n",
    "                        print(f\"Continuing entity: {current_entity_type}, token: {token}\")\n",
    "                else:\n",
    "                    # Finalize the previous entity due to break\n",
    "                    merged_bbox, merged_token, entity_class = merge_entity(\n",
    "                        bbox_stack, token_stack, current_entity_type\n",
    "                    )\n",
    "                    if merged_bbox:\n",
    "                        merged_boxes.append(merged_bbox)\n",
    "                        merged_tokens.append(merged_token)\n",
    "                        merged_classes.append(entity_class)\n",
    "                        if verbose: print(f\"Finalized entity (due to break): {entity_class} -> '{merged_token}'\")\n",
    "                    \n",
    "                    # Start new entity with the current token\n",
    "                    bbox_stack = [current_bbox]\n",
    "                    token_stack = [str(token)]\n",
    "                    # current_entity_type remains the same (as we're still in the same entity type)\n",
    "                    if verbose:\n",
    "                        print(f\"Starting new entity (after break): {current_entity_type}, token: {token}\")\n",
    "\n",
    "    # Final check: Merge any remaining entity after the loop\n",
    "    if bbox_stack:\n",
    "        merged_bbox, merged_token, entity_class = merge_entity(\n",
    "            bbox_stack, token_stack, current_entity_type\n",
    "        )\n",
    "        if merged_bbox:\n",
    "            merged_boxes.append(merged_bbox)\n",
    "            merged_tokens.append(merged_token)\n",
    "            merged_classes.append(entity_class)\n",
    "            if verbose: print(f\"Finalized entity (end of list): {entity_class} -> '{merged_token}'\")\n",
    "    \n",
    "    # Final validation for visualization function compatibility\n",
    "    if not (len(merged_boxes) == len(merged_tokens) == len(merged_classes)):\n",
    "        print(\"Error: Length mismatch after merging! Check verbose logs.\")\n",
    "        print(f\"Boxes: {len(merged_boxes)}, Tokens: {len(merged_tokens)}, Classes: {len(merged_classes)}\")\n",
    "    \n",
    "    return merged_boxes, merged_tokens, merged_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def pixel_bbox_to_percent(\n",
    "    bbox: Tuple[int, int, int, int], image_width: int, image_height: int\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Zamienia bbox w pikselach (x1,y1,x2,y2)\n",
    "    na wartości procentowe (x%, y%, width%, height%).\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x_pct: float = (x1 / image_width) * 100.0\n",
    "    y_pct: float = (y1 / image_height) * 100.0\n",
    "    width_pct: float = ((x2 - x1) / image_width) * 100.0\n",
    "    height_pct: float = ((y2 - y1) / image_height) * 100.0\n",
    "    return x_pct, y_pct, width_pct, height_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "ROOT_DIR = \"/Users/user/Projects/AI_Osrodek/\"\n",
    "LLM_ANNOTATIONS_DIR = \"data/llm_annotations\"\n",
    "SCHEMATISM_DIR = \"data/schematyzmy\"\n",
    "schematims_to_synchronize = [schematism for schematism in os.listdir(os.path.join(ROOT_DIR, LLM_ANNOTATIONS_DIR)) \n",
    "                           if not schematism.startswith(\".\") and os.path.isdir(os.path.join(ROOT_DIR, LLM_ANNOTATIONS_DIR, schematism))]\n",
    "\n",
    "for schematism in schematims_to_synchronize:\n",
    "    print(\"Processing schematism:\", schematism)\n",
    "    filters = Filters.create(Filters.AND, [Filters.item(\"tasks:storage_filename\", Operator.CONTAINS, Type.String, Filters.value(schematism))])\n",
    "\n",
    "    tasks = project.get_tasks(filters=filters)\n",
    "    tasks_dict = {task[\"storage_filename\"].split(\"/\")[-1]: task for task in tasks}\n",
    "    \n",
    "    for annotation_json in os.listdir(os.path.join(ROOT_DIR, LLM_ANNOTATIONS_DIR, schematism)):\n",
    "        print(\"Processing annotation:\", annotation_json)\n",
    "        \n",
    "        if not annotation_json.endswith(\".json\"):\n",
    "            continue\n",
    "            \n",
    "        with open(os.path.join(ROOT_DIR, LLM_ANNOTATIONS_DIR, schematism, annotation_json), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            words, bboxes, labels = data[\"words\"], data[\"bboxes\"], data[\"labels\"]\n",
    "\n",
    "        # Get the corresponding image filename\n",
    "        image_filename = annotation_json.replace(\".json\", \".jpg\")\n",
    "        \n",
    "        # Check if we have this image in our tasks\n",
    "        if image_filename not in tasks_dict:\n",
    "            print(f\"Warning: No task found for {image_filename}\")\n",
    "            continue\n",
    "            \n",
    "        # Get the specific task for this annotation\n",
    "        task = tasks_dict[image_filename]\n",
    "        image_path = os.path.join(ROOT_DIR, SCHEMATISM_DIR, schematism, image_filename)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            image_width, image_height = image.size\n",
    "            image = image.convert(\"RGB\")\n",
    "            \n",
    "            merged_bboxes, merged_sentences, merged_classes = merge_bio_entities(\n",
    "                bboxes,\n",
    "                labels,\n",
    "                words,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            annotated_image = visualize_tokens_with_labels(\n",
    "                image_path=image_path,\n",
    "                words=merged_sentences,\n",
    "                bboxes=merged_bboxes,\n",
    "                labels=merged_classes,\n",
    "                # output_path=os.path.join(ROOT_DIR, SCHEMATISM_DIR, schematism, image_filename.replace(\".jpg\", \"_visualized.jpg\")),\n",
    "            )\n",
    "\n",
    "            display(annotated_image)\n",
    "\n",
    "            # Create results for this specific task\n",
    "            current_task_results = []\n",
    "            for bbox, class_name in zip(merged_bboxes, merged_classes):\n",
    "                x_percent, y_percent, width_percent, height_percent = (\n",
    "                    pixel_bbox_to_percent(\n",
    "                        bbox=bbox,\n",
    "                        image_width=image_width,\n",
    "                        image_height=image_height,\n",
    "                    )\n",
    "                )\n",
    "                result_item = {\n",
    "                    \"type\": \"rectanglelabels\",\n",
    "                    \"from_name\": \"label\",  # Make sure this matches your Label Studio config\n",
    "                    \"to_name\": \"image\",    # Make sure this matches your Label Studio config\n",
    "                    \"original_width\": image_width,\n",
    "                    \"original_height\": image_height,\n",
    "                    \"image_rotation\": 0,\n",
    "                    \"value\": {\n",
    "                        \"rectanglelabels\": [class_name],\n",
    "                        \"x\": x_percent,\n",
    "                        \"y\": y_percent,\n",
    "                        \"width\": width_percent,\n",
    "                        \"height\": height_percent,\n",
    "                    },\n",
    "                    \"score\": 1.0,\n",
    "                }\n",
    "                current_task_results.append(result_item)\n",
    "\n",
    "            # Add a single prediction for this task\n",
    "            prediction = {\n",
    "                \"task\": task[\"id\"],\n",
    "                \"result\": current_task_results,\n",
    "                \"score\": 0.9,  # Overall confidence score\n",
    "                \"model_version\": \"llm_mistral_0.0.1\"\n",
    "            }\n",
    "            \n",
    "            # Upload prediction for this specific task\n",
    "            upload_response = project.create_predictions(\n",
    "                predictions=[prediction],\n",
    "            )\n",
    "            print(f\"Upload response for task {task['id']}: {upload_response}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
