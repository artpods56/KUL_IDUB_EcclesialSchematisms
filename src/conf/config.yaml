model:
  checkpoint: microsoft/layoutlmv3-base

run:
  device: cuda

dataset:
  name: "artpods56/EcclesialSchematisms"
  force_download: false
  seed: 42
  eval_size: 0.2
  test_size: 0.1

  image_column_name: "image_pil"
  text_column_name: "words"
  boxes_column_name: "bboxes"
  label_column_name: "labels"

  schematisms_to_train:
    - "wloclawek_1872"
    - "wloclawek_1873"
    - "tarnow_1870"
    - "chelmno_1871"

  classes_to_remove:
    - "B-settlement_classification" 
    - "I-settlement_classification"
    - "B-material" 
    - "I-material"
    - "B-deanery"
    - "I-deanery"

processor:
  checkpoint: microsoft/layoutlmv3-large
  max_length: 512


focal_loss:
  alpha: 1.0
  gamma: 1.0  # Changed from 1.0 to 2.0

training:
  output_dir: "test_focal"
  max_steps: 8000
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 2e-5
  eval_strategy: "steps"
  eval_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "eval_overall_f1"
  report_to: "wandb"
  run_name: "layoutlmv3-large-focal"
  logging_strategy: "steps"
  logging_steps: 100
  logging_dir: "logs_focal"
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 2
  fp16: true

metrics:
  return_entity_level_metrics: true

defaults:
  - _self_
  - wandb
