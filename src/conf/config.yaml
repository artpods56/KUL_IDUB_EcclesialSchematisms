model:
  checkpoint: microsoft/layoutlmv3-large

dataset:
  name: "artpods56/EcclesialSchematisms"
  seed: 42
  eval_size: 0.2

  image_column_name: "image_pil"
  text_column_name: "words"
  boxes_column_name: "bboxes"
  label_column_name: "labels"


processor:
  checkpoint: microsoft/layoutlmv3-large
  max_length: 512


focal_loss:
  alpha: 1.0
  gamma: 2.0  # Changed from 1.0 to 2.0

training:
  output_dir: "test_focal"
  max_steps: 300
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 5e-5
  eval_strategy: "steps"
  eval_steps: 50
  load_best_model_at_end: true
  metric_for_best_model: "eval_overall_f1"
  report_to: "wandb"
  run_name: "layoutlmv3-large-focal"
  logging_strategy: "steps"
  logging_steps: 100
  logging_dir: "logs_focal"
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 2

metrics:
  return_entity_level_metrics: true

defaults:
  - _self_
  - wandb
